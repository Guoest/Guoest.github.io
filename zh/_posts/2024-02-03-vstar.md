---
layout: post
title: 深入了解V* - 超越GPT-4V的视觉搜索
date: 2024-02-03
categories: [tech]
lang: zh
permalink: /zh/tech/2024/02/03/vstar.html
---

<div class="table-of-contents" markdown="1">
### 目录
- [简介](#简介)
- [通俗解释](#通俗解释)
- [与GPT-4V的比较](#与gpt-4v的比较)
- [实战演示](#实战演示)
- [V*真的比GPT-4V更好吗](#v真的比gpt-4v更好吗)
</div>

## 简介

GPT-4V一直是迄今为止最好的视觉模型，但随着V*和SEAL（Show, SEArch, and TelL）框架的出现，一个新时代来临了。V*在处理高分辨率图像细节方面提供了更准确的方法，而这正是GPT-4V会失败的地方。

这里我链接了项目网站和论文：

- [项目网站](https://vstar-seal.github.io/)
- [论文](https://arxiv.org/abs/2312.14135)

## 通俗解释

![](/images/vstar-architecture.png "所提出的SEAL框架的实例化。左侧部分代表VQA LLM，它利用视觉工作记忆中的所有数据来回答问题。右侧，我们展示了V*视觉搜索算法的操作流程。（参考 https://vstar-seal.github.io/）")

我不会深入探讨框架的具体细节和支撑模型的数学原理，因为这些可以在项目网站和附带论文中找到。相反，我旨在用简单的术语描述方法论，这是我喜欢的学习方式。

简单来说，V*搜索模拟了人类进行视觉搜索任务的方式：

- 首先，我们识别我们正在寻找的对象的特征，例如饮料的颜色。
- 接下来，我们推断对象最可能的位置；例如，饮料很可能在桌子上找到。
- 随后，我们首先侦察大致位置，比如桌子。
- 找到大致区域后，我们缩小焦点，仔细检查周围环境以获取有关那里对象的更多细节，例如识别饮料及其颜色。

鉴于大型语言模型（LLM）具有理解任务并相应地进行上下文化的能力，我们可以指导LLM执行上述步骤。重复此过程，直到有强烈的信心已找到目标。这本质上就是V*：LLM引导的视觉搜索。


## 与GPT-4V的比较

![](/images/gpt4-compare.png "与GPT-4V的比较。在这种情况下，GPT-4V答错了，而SEAL V*是正确的。（来源 https://vstar-seal.github.io/）")

尽管GPT-4V取得了进步，但在分析高分辨率图像和复杂视觉任务时表现不佳，通常将图像作为一个整体处理，从而导致不准确。它缺乏复杂视觉分析所需的细致、选择性焦点，在细节和深度上下文理解方面存在困难。

相反，SEAL V*通过战略性地识别和专注于特定图像部分，迭代地完善其搜索而表现出色。这种有针对性的方法使V*在详细的视觉任务中超越了GPT-4V的性能。

## 实战演示

![](/images/vstar-pseudo-code.png "V*伪代码")

我将逐步可视化SEAL V*的工作原理。使用上面的猿吉他图像进行演示：

1. 查看整个图像并提问"猿拿着的乐器是什么？"。决策：`需要进行视觉搜索以搜索：猿拿着的乐器。`
![](/images/vstar/step_1.jpg "步骤1")
2. 将图像分成4个部分，并按目标可能性排序，将它们添加到搜索队列中。搜索队列顶部的图像（可能性最大且可能性>阈值）。决策：`搜索左下角`并生成目标的概率热图。
![](/images/vstar/step_2.jpg "步骤2")
![](/images/vstar/step_2_heatmap.jpg "步骤2热图")
热图聚焦在汽车周围，进一步将左下图像分成更小的4个部分，选择高热度（汽车）的部分并添加到队列中以供后续搜索。
3. 移回右下角，并生成目标的概率热图。同样，热图上有一些东西，进一步分成4个部分并添加到搜索队列。
![](/images/vstar/step_3.jpg "步骤3")
![](/images/vstar/step_3_heatmap.jpg "步骤3热图")
4. 搜索队列的下一项，回到汽车图像，概率未超过阈值，继续前进。
![](/images/vstar/step_4.jpg "步骤4")
5. 搜索队列的下一项，找到`猿`，返回结果。
![](/images/vstar/step_5.jpg "步骤5")
![](/images/vstar/search_result.jpg "搜索在这里停止")
![](/images/vstar/result.png "最终位置")

这是程序的输出。请注意，参数都可以调整：
- 置信度阈值，决定是否继续搜索或有足够信心返回结果
- 图像裁剪大小，每次裁剪可以有多小，在最小尺寸处停止，例如255x255
- 图像裁剪数量，决定将图像分成多少个部分。

```python

CPU times: user 42.5 s, sys: 2.77 s, total: 45.2 s
Wall time: 42.5 s
('Need to conduct visual search to search for: instrument held by an ape.',
 'Targets located after search: instrument held by an ape.',
 array([[[255, 255, 255],
         [255, 255, 255],
         [255, 255, 255],
         ...,
         [255, 255, 255],
         [255, 255, 255],
         [255, 255, 255]]], dtype=uint8),
 'The instrument held by an ape is a guitar.')

```


## V*真的比GPT-4V更好吗

在我看来，只有在有限的条件下。

这是因为准确性的代价很高。这种引导搜索的方法让我想起了langchain和AutoGPT，你要求LLM逐步思考，在每一步中，你反复要求它执行进一步的操作。结果？如你所见，运行时间非常长。这种延迟是由于多次调用LLM来执行操作。在某些情况下，步骤可以增长到20多个，运行时间将是几分钟！

理论上，底层LLM LLAVA-7B的性能不如GPT-4V。所以，我用GPT-4V进行了类似的实验，我遵循SEAL框架，将图像分成更小的部分并提问；以下是结果：

![](/images/vstar/gpt2.png "在整个图像上询问GPT-4V，它返回了错误答案")

![](/images/vstar/gpt1.png "在图像的子部分上询问GPT-4V，它返回了正确答案")

因此，显然GPT-4V可以通过详细的裁剪图像产生更准确的结果。此外，用GPT-4V替换V*中的LLM可能会进一步提高性能。


## 结论

虽然声称SEAL V*超越GPT-4V可能会成为引人注目的头条新闻，但这一优势是以对每个图像的较小部分进行重复搜索的高成本为代价的。

一个潜在的优化可能涉及使用LLM来引导搜索，结合像YOLO这样的CNN进行对象检测。这可以提高效率，因为仅依赖LLM可能会很慢且昂贵。然而，这种方法需要为特定任务训练的CNN，这限制了其通用性。

尽管如此，SEAL（Show, SEArch, and TelL）框架代表了一种模仿人类行为的创新视觉搜索方法。它在对高分辨率图像进行详细搜索方面表现出优越的准确性。这种方法可能会为新的视觉搜索技术铺平道路，可能导致既快速又精确的解决方案。

感谢阅读！